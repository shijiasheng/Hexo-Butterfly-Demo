<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>KSVD</title>
      <link href="2020/10/31/KSVD/"/>
      <url>2020/10/31/KSVD/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>如果你认出了到这篇博客的封面，说明你很有眼光哦！（请耐心阅读，文末给出答案）。</p><p>本博客代码为KSVD的MATLAB代码，增加了详细的注释，尽可能地每个函数都写了注释（基本上每行都有注释）。</p><p>地址：<a href="https://github.com/sjspingfandeshijie/KSVD">https://github.com/sjspingfandeshijie/KSVD</a></p><h1 id="KSVD算法介绍"><a href="#KSVD算法介绍" class="headerlink" title="KSVD算法介绍"></a>KSVD算法介绍</h1><p>KSVD算法，顾名思义，就是K个SVD算法，为什么要用SVD算法呢，稍后会讲。</p><p>KSVD算法本质上是字典的学习。这里的字典其实可以理解为一个翻译的工具，当我们拥有一个原始样本，我们用Y来表示，那么我们需要尽可能的用稀疏矩阵X来表示Y，也就是这个翻译的过程，用到了我们的字典Dictionary。对于字典Dictionary来说，它的每列，我们定义为原子（atom）。</p><p>所以如果我们需要一个Y的稀疏表示，则KSVD是一个功能比较强大的方法。所以这个方法时常用在图像去噪当中。</p><p>我们目前知道了原始样本Y，为了获取Y的稀疏表示，需要人为构建D和X，D为字典Dictionary，X为稀疏表示，理想的情况是Y=D * X，但是一般来说做不到，所以需要尽可能保证Y和D * X接近，同时需要满足X足够稀疏。数学表达式如下：</p><p><img src="https://i.loli.net/2020/10/31/9pC6ROvYcsAumJW.png" alt="image-20201031173854731"></p><p>或者</p><p><img src="https://i.loli.net/2020/10/31/EJQrgjYwk8z6nsx.png" alt="image-20201031174410655"></p><p>我们知道电脑只能每次实现一个功能，如何尽可能同时满足这两个要求呢？那么需要不断地迭代，在迭代过程中不断趋近于最优解。在每次迭代中都需要完成两个步骤：一个是稀疏编码，一个是字典更新。</p><p>首先是Y=D * X，Y是已知的矩阵，D是字典dictionary，X是未知的系数矩阵。</p><p>稀疏编码环节中要将系数矩阵尽可能地稀疏，而字典学习环节中则为在系数矩阵已知的情况下努力将Dictionary学习出来，使得Dictionary接近最理想的状态（Y-D*X尽可能小）。即前文的数学表达。</p><h1 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h1><h2 id="稀疏编码"><a href="#稀疏编码" class="headerlink" title="稀疏编码"></a>稀疏编码</h2><p>稀疏编码（Sparse Coding Stage）：使用OMP算法，进行对于系数矩阵的构建，满足字典足够满足要求，同时x足够稀疏。</p><p>Use any pursuit algorithm to compute the representation vectors alphaij for each path RijX, by approximating the solution of</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">% find the coefficients 发现系数</span></span><br><span class="line"><span class="keyword">if</span> (param.errorFlag==<span class="number">0</span>)  <span class="comment">%param.errorFlag = 1;   </span></span><br><span class="line">    CoefMatrix = OMP([FixedDictionaryElement,Dictionary],Data, param.L); </span><br><span class="line">    <span class="comment">%size(Data,2)=249*249</span></span><br><span class="line"><span class="keyword">else</span>  </span><br><span class="line">    <span class="comment">%param.errorGoal = sigma*C;</span></span><br><span class="line">    CoefMatrix = OMPerr([FixedDictionaryElement,Dictionary],Data, param.errorGoal);</span><br><span class="line">    <span class="comment">%param.errorGoal = sigma*C;   稀疏矩阵</span></span><br><span class="line">    param.L = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><h2 id="字典更新"><a href="#字典更新" class="headerlink" title="字典更新"></a>字典更新</h2><p>字典更新（Dictionary Update Stage）：</p><p>For each column in D, update it by…</p><p>Find the set of patches that use this atom        </p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> <span class="built_in">j</span> = rPerm</span><br><span class="line">    <span class="comment">% 初步估计是每次只更新一列</span></span><br><span class="line">    [betterDictionaryElement,CoefMatrix,addedNewVector] = I_findBetterDictionaryElement(Data,...</span><br><span class="line">        [FixedDictionaryElement,Dictionary],<span class="built_in">j</span>+<span class="built_in">size</span>(FixedDictionaryElement,<span class="number">2</span>),...</span><br><span class="line">        CoefMatrix ,param.L);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">% 只更新了第j列</span></span><br><span class="line">    Dictionary(:,<span class="built_in">j</span>) = betterDictionaryElement;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">% param.preserveDCAtom暂时=0，作用有待考证，这段代码不运行</span></span><br><span class="line">    <span class="keyword">if</span> (param.preserveDCAtom)</span><br><span class="line">        tmpCoef = FixedDictionaryElement\betterDictionaryElement;</span><br><span class="line">        Dictionary(:,<span class="built_in">j</span>) = betterDictionaryElement - FixedDictionaryElement*tmpCoef;</span><br><span class="line">        Dictionary(:,<span class="built_in">j</span>) = Dictionary(:,<span class="built_in">j</span>)./<span class="built_in">sqrt</span>(Dictionary(:,<span class="built_in">j</span>)&#x27;*Dictionary(:,<span class="built_in">j</span>));<span class="comment">%去掉一列以后重新归一化</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">% 简单的计数器</span></span><br><span class="line">    replacedVectorCounter = replacedVectorCounter+addedNewVector;<span class="comment">%实验证明（针对w.jpg图像），值累加了一次</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p>其中，调用了I_findBetterDictionaryElement函数，这个函数可以理解为一列一列得更新字典。</p><p>先到这里了，最近《数码宝贝》20周年大电影上线咯！正如封面所示，我惊奇地发现电影里数码宝贝和数码世界都是用python写的，大家一定要去影院多多支持！</p><p>虽然长大了，但还是那群被选召的孩子啊！</p><p><img src="https://i.loli.net/2020/10/31/LGVBxJi3PK2k4m1.png" alt="image-20201031181706617"></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>TextCNN</title>
      <link href="2020/10/15/TextCNN/"/>
      <url>2020/10/15/TextCNN/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>CNN的全称是Convolutional Neural Networks，中文名称是卷积神经网络。通过卷积的方式对于图像文字进行识别。</p><p>本文主要介绍Text CNN的原理和代码实现（超级详解）！</p><h1 id="Text-CNN的原理"><a href="#Text-CNN的原理" class="headerlink" title="Text CNN的原理"></a>Text CNN的原理</h1><h2 id="矩阵是怎样炼成的"><a href="#矩阵是怎样炼成的" class="headerlink" title="矩阵是怎样炼成的"></a>矩阵是怎样炼成的</h2><p>对于图片进行CNN的图像识别，首先是将图片进行矩阵化，像素化，图像本质上是一个矩阵。如果是黑白图像，则本质上就是一个矩阵，矩阵上的每一个元素为从0到255间取值。</p><p>将图片还原成一个矩阵之后则可以通过CNN的卷积池化等基本操作来对于图片进行分类识别了。</p><p>对于Text CNN也是一样的。我们尝试着让训练后的模型能够识别出这段文字的类别。比如，将新闻分为政治新闻，财经新闻，体育新闻，娱乐新闻等等。类比于图片，我们需要为一句话构建一个像图片一样的矩阵。但是怎么做到呢？<strong>词向量</strong>！</p><h2 id="词向量的爱恨情仇"><a href="#词向量的爱恨情仇" class="headerlink" title="词向量的爱恨情仇"></a>词向量的爱恨情仇</h2><p>所以何谓词向量？首先由于这个算法是歪果仁发明的，而英语采用的是一个词一个词进行分类的，那么对于中国人来说，我们的语句构成的基本单位是字，所以之后虽然说的是词向量，我们其实是按照词进行的标识。（当然，你也可以进行词分类，Github上有成型的进行分词的代码模型，叫做<a href="https://github.com/fxsjy/jieba">结巴分词</a>，结巴分词可以将一句话以词当作基本单位进行构建词向量，理论上讲语句分类效果应该更好，但我没有实践过，纯属推测。）</p><p>当你构建词向量之后，在脑海中想象一下，将一句话竖过来，那么我们可以构建一个矩阵，矩阵每行对应着一个词向量。当我们构建好这个矩阵之后，即可进行神经卷积了！</p><p>真不知道是哪个大佬真聪明智慧，想到的这种方法，解决了多少莘莘学子（误。</p><h2 id="卷积神经网络的构建"><a href="#卷积神经网络的构建" class="headerlink" title="卷积神经网络的构建"></a>卷积神经网络的构建</h2><p>卷积神经网络，顾名思义，最重要的就是卷积，何为卷积？个人理解：卷，就是内卷（误。</p><p>卷积，主要是积，我们设置一个三维的卷积核（也有描述为一个卷积核为一个矩阵，共有多少个卷积核，即通道数，channel数）。让一个卷积核和构建的句子的矩阵相乘，从左往右，从上往下，能够构建出一个新的小矩阵。</p><p>池化，池化为缩小矩阵，通过例如取最大值的方式进行。</p><h1 id="Text-CNN的实现"><a href="#Text-CNN的实现" class="headerlink" title="Text CNN的实现"></a>Text CNN的实现</h1><h2 id="源代码拷贝"><a href="#源代码拷贝" class="headerlink" title="源代码拷贝"></a>源代码拷贝</h2><p>代码来自于：<a href="https://github.com/649453932/Chinese-Text-Classification-Pytorch">Chinese-Text-Classification-Pytorch</a></p><p>代码通过pytorch进行神经卷积的构建</p><p>我没有怎么改动代码，主要是把结构捋顺，然后注释写的比较详细，很适合像我一样的菜鸡阅读，用jupyter notebook呈现出来。</p><p>这是我加了不少注释的版本：<a href="https://github.com/sjspingfandeshijie/NLP/blob/main/Chinese-Text-Classification-Pytorch/textCNN.ipynb">jupyter notebook</a></p><h2 id="代码详解"><a href="#代码详解" class="headerlink" title="代码详解"></a>代码详解</h2><h3 id="训练集，测试集简介"><a href="#训练集，测试集简介" class="headerlink" title="训练集，测试集简介"></a>训练集，测试集简介</h3><p>采用的测试集和训练集都是采用的清华大学的研究的数据集<a href="http://thuctc.thunlp.org/">THUCNews</a>，主要将新闻标题进行分类。分为10类。</p><p>规格如下：</p><p>​    中华女子学院：本科层次仅1专业招男生    3<br>​    两天价网站背后重重迷雾：做个网站究竟要多少钱    4<br>​    东5环海棠公社230-290平2居准现房98折优惠    1<br>​    卡佩罗：告诉你德国脚生猛的原因 不希望英德战踢点球    7<br>​    82岁老太为学生做饭扫地44年获授港大荣誉院士    5<br>​    记者回访地震中可乐男孩：将受邀赴美国参观    5<br>​    传郭晶晶欲落户香港战伦敦奥运 装修别墅当婚房    1<br>​    《赤壁OL》攻城战诸侯战硝烟又起    8<br>​    “手机钱包”亮相科博会    4</p><p>分类标准如下：</p><p>​    finance<br>​    realty<br>​    stocks<br>​    education<br>​    science<br>​    society<br>​    politics<br>​    sports<br>​    game<br>​    entertainment</p><h3 id="构建词汇和数据集"><a href="#构建词汇和数据集" class="headerlink" title="构建词汇和数据集"></a>构建词汇和数据集</h3><p>词汇的构建，首先需要保证训练的数据集中每个字要保证它的独特性，即我们需要给每个字进行一个独一无二的编号。采用的是UTF-8编码方式（这个编码比较全面，有些奇怪符号也能识别记录下来，这是我从另一门款爷的数据仓库中了解到的小知识，什么？你不知道款爷？<a href="https://sse.tongji.edu.cn/Data/View/3142">点击了解款爷</a>）。</p><p>其中第五行的for循环，主要是将数据集中的内容导入，清洗，包括去掉空格，去掉tab，然后进行字的筛选录入（如果已经存在就不导入）。这样，词汇的词典就构建完成了！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_vocab</span>(<span class="params">file_path, tokenizer, max_size, min_freq</span>):</span></span><br><span class="line">    <span class="comment">#词汇字典</span></span><br><span class="line">    vocab_dic = &#123;&#125;</span><br><span class="line">    <span class="keyword">with</span> open(file_path, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;UTF-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> tqdm(f):</span><br><span class="line">            lin = line.strip()</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> lin:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            content = lin.split(<span class="string">&#x27;\t&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">            <span class="keyword">for</span> word <span class="keyword">in</span> tokenizer(content):</span><br><span class="line">                vocab_dic[word] = vocab_dic.get(word, <span class="number">0</span>) + <span class="number">1</span></span><br><span class="line">        vocab_list = sorted([_ <span class="keyword">for</span> _ <span class="keyword">in</span> vocab_dic.items() <span class="keyword">if</span> _[<span class="number">1</span>] &gt;= min_freq], key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)[:max_size]</span><br><span class="line">        vocab_dic = &#123;word_count[<span class="number">0</span>]: idx <span class="keyword">for</span> idx, word_count <span class="keyword">in</span> enumerate(vocab_list)&#125;</span><br><span class="line">        vocab_dic.update(&#123;UNK: len(vocab_dic), PAD: len(vocab_dic) + <span class="number">1</span>&#125;)</span><br><span class="line">    <span class="keyword">return</span> vocab_dic</span><br></pre></td></tr></table></figure><p>数据集的构建，则将词语进行更多的操作。比如将每个字进行了出现频率上的统计。</p><p>同时构建三个数据集，分别是训练集，验证集，测试集。训练集用于训练模型，验证集用于每个epoch的验证，测试集用于最后测试效果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_dataset</span>(<span class="params">config, ues_word</span>):</span></span><br><span class="line">    <span class="keyword">if</span> ues_word:</span><br><span class="line">        tokenizer = <span class="keyword">lambda</span> x: x.split(<span class="string">&#x27; &#x27;</span>)  <span class="comment"># 以空格隔开，word-level</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        tokenizer = <span class="keyword">lambda</span> x: [y <span class="keyword">for</span> y <span class="keyword">in</span> x]  <span class="comment"># char-level</span></span><br><span class="line">    <span class="comment"># 判断并扩充词字典</span></span><br><span class="line">    <span class="keyword">if</span> os.path.exists(config.vocab_path):</span><br><span class="line">        vocab = pkl.load(open(config.vocab_path, <span class="string">&#x27;rb&#x27;</span>))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        vocab = build_vocab(config.train_path, tokenizer=tokenizer, max_size=MAX_VOCAB_SIZE, min_freq=<span class="number">1</span>)</span><br><span class="line">        pkl.dump(vocab, open(config.vocab_path, <span class="string">&#x27;wb&#x27;</span>))</span><br><span class="line">    <span class="comment"># 多少个字词</span></span><br><span class="line">    print(<span class="string">f&quot;Vocab size: <span class="subst">&#123;len(vocab)&#125;</span>&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 加载数据集</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load_dataset</span>(<span class="params">path, pad_size=<span class="number">32</span></span>):</span></span><br><span class="line">        contents = []</span><br><span class="line">        <span class="keyword">with</span> open(path, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;UTF-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> tqdm(f):     <span class="comment"># tqdm进度条</span></span><br><span class="line">                lin = line.strip()   <span class="comment"># strip()去掉了首尾的空格</span></span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> lin:          <span class="comment">#判断不是空的</span></span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                content, label = lin.split(<span class="string">&#x27;\t&#x27;</span>)    <span class="comment"># content去掉tab</span></span><br><span class="line">                words_line = []</span><br><span class="line">                token = tokenizer(content)          <span class="comment"># tokenizer为分词器，token变成不同数字统计类似，模式示例&#123;&#x27;下&#x27;: 1, &#x27;北京&#x27;: 1, &#x27;今天&#x27;: 2, &#x27;雨&#x27;: 1, &#x27;了&#x27;: 2, &#x27;我&#x27;: 1, &#x27;加班&#x27;: 1&#125;</span></span><br><span class="line">                seq_len = len(token)                <span class="comment"># token的长度，多少个词</span></span><br><span class="line">                <span class="comment"># 之前没有的就扩充</span></span><br><span class="line">                <span class="keyword">if</span> pad_size:</span><br><span class="line">                    <span class="keyword">if</span> len(token) &lt; pad_size:</span><br><span class="line">                        token.extend([PAD] * (pad_size - len(token)))</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        token = token[:pad_size]</span><br><span class="line">                        seq_len = pad_size</span><br><span class="line">                <span class="comment"># word to id</span></span><br><span class="line">                <span class="keyword">for</span> word <span class="keyword">in</span> token:</span><br><span class="line">                    words_line.append(vocab.get(word, vocab.get(UNK)))</span><br><span class="line">                contents.append((words_line, int(label), seq_len))</span><br><span class="line">        <span class="keyword">return</span> contents  <span class="comment"># [([...], 0), ([...], 1), ...]</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 三种数据集的加载</span></span><br><span class="line">    train = load_dataset(config.train_path, config.pad_size) <span class="comment"># 训练集集</span></span><br><span class="line">    dev = load_dataset(config.dev_path, config.pad_size) <span class="comment"># 验证集</span></span><br><span class="line">    test = load_dataset(config.test_path, config.pad_size) <span class="comment"># 测试集</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#返回的是词典，三种数据集</span></span><br><span class="line">    <span class="keyword">return</span> vocab, train, dev, test</span><br></pre></td></tr></table></figure><h3 id="构建词向量"><a href="#构建词向量" class="headerlink" title="构建词向量"></a>构建词向量</h3><p>词向量通过Embedding函数进行，如前文所述，完成词向量的构建。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        super(Model, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> config.embedding_pretrained <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.embedding = nn.Embedding.from_pretrained(config.embedding_pretrained, freeze=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.embedding = nn.Embedding(config.n_vocab, config.embed, padding_idx=config.n_vocab - <span class="number">1</span>)<span class="comment">#Embedding函数（词的数量，</span></span><br><span class="line">        self.convs = nn.ModuleList(</span><br><span class="line">            [nn.Conv2d(<span class="number">1</span>, config.num_filters, (k, config.embed)) <span class="keyword">for</span> k <span class="keyword">in</span> config.filter_sizes])</span><br><span class="line">        self.dropout = nn.Dropout(config.dropout)</span><br><span class="line">        self.fc = nn.Linear(config.num_filters * len(config.filter_sizes), config.num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 卷积和池化</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">conv_and_pool</span>(<span class="params">self, x, conv</span>):</span></span><br><span class="line">        x = F.relu(conv(x)).squeeze(<span class="number">3</span>) <span class="comment"># 卷积</span></span><br><span class="line">        x = F.max_pool1d(x, x.size(<span class="number">2</span>)).squeeze(<span class="number">2</span>) <span class="comment"># 池化</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment"># （嵌入层），数量，维度</span></span><br><span class="line">        out = self.embedding(x[<span class="number">0</span>])</span><br><span class="line">        out = out.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        out = torch.cat([self.conv_and_pool(out, conv) <span class="keyword">for</span> conv <span class="keyword">in</span> self.convs], <span class="number">1</span>)</span><br><span class="line">        out = self.dropout(out) <span class="comment"># 随机失活</span></span><br><span class="line">        out = self.fc(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure><p>然后就是CNN的固定操作了，卷积，池化，吧啦吧啦。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 卷积和池化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_and_pool</span>(<span class="params">self, x, conv</span>):</span></span><br><span class="line">    x = F.relu(conv(x)).squeeze(<span class="number">3</span>) <span class="comment"># 卷积</span></span><br><span class="line">    x = F.max_pool1d(x, x.size(<span class="number">2</span>)).squeeze(<span class="number">2</span>) <span class="comment"># 池化</span></span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="2020/10/02/hello-world/"/>
      <url>2020/10/02/hello-world/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>Hello world!</p><p>This is my first blog.</p><p>And this is me:</p><p><img src="https://i.loli.net/2020/10/16/qd9Fxea4t1IjHUW.jpg" alt="sjs"></p><p>Hope you enjoy your time in my blog.</p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
