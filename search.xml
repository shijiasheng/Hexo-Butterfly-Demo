<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>MATLAB Install Tutorial in Linux</title>
      <link href="2021/04/28/MATLAB%20Install%20Tutorial%20in%20Linux/"/>
      <url>2021/04/28/MATLAB%20Install%20Tutorial%20in%20Linux/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>很久没更新了，这次要更新一下了，这么说吧，互联网上目前我还没有看到比我这个更详细且有效的MATLAB安装及打开教程。我在多次安装的过程中出现了很多很多的问题，多到离谱，网上查到的信息没有什么有帮助的，真的不知道各位大佬都是怎么安装MATLAB的，真的太强了，像我这样的菜鸡费了好大的力气才成功。</p><p>接下来我会详细介绍一下如何在Linux系统上安装且打开MATLAB，我这篇博客主要面向没有或者几乎没有Linux系统操作经历的同学。</p><h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><p>首先是我们要获取MATLAB的iso文件，支持正版，资源自找。我的这个文件名是：R2020a_Linux.iso，是2020版的，之前的版本还有两个挂载文件的，我没有亲自试过，但是我觉得用最新版的肯定不会有什么问题，因为MATLAB大部分都是向前兼容的，比如很久之前的matlab代码虽然会有红线报错，但是依然可以跑起来。</p><p>首先将获取的iso文件放在一个地址，我们称之为<strong>地址1</strong>。</p><p>其次我们需要给这个iso文件挂载，也就是安装。我们需要设置或者新建一个<strong>地址2</strong>。</p><p>建议所有的地址都在<strong>固态硬盘</strong>上，而不是<strong>机械硬盘</strong>，不然很有可能<strong>宕机</strong>！</p><p>在确定所有的地址后，我们就要开始了！</p><p><strong>切记Linux系统的地址都是’/‘开头的！</strong></p><p>首先我们需要进入管理员，这样就不太会出现奇奇怪怪的问题了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo -i</span><br></pre></td></tr></table></figure><p>然后我们就要挂载了，可以直接按照我的这个公式套。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo mount -t auto -o loop [地址1]&#x2F;R2020a_Linux.iso [地址2]</span><br></pre></td></tr></table></figure><p>然后</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd [地址2]</span><br></pre></td></tr></table></figure><p>进入到挂载的地址之后，就可以安装了！</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo .&#x2F;install</span><br></pre></td></tr></table></figure><p>这样就可以开始安装MATLAB了，进入了和其它系统一样的程序了！</p><p>另外需要注意的是，LInux系统打开这个程序，<strong>需要设定一个用户名，千万千万要用root！</strong>原因如下：</p><p>如果需要重新安装一些安装包，如果不是root还是会出现很多问题的。</p><p>然后中间会有一个程序安装在哪个地方，这里就是我们的<strong>地址3</strong>，比如我的就是/usr/local/MATLAB/R2020a</p><h1 id="打开"><a href="#打开" class="headerlink" title="打开"></a>打开</h1><p>首先还是进入管理员，这样真的可以省很多事情</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo -i</span><br></pre></td></tr></table></figure><p>然后是打开我们安装的地址</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd &#x2F;usr&#x2F;local&#x2F;MATLAB&#x2F;R2020a&#x2F;bin</span><br></pre></td></tr></table></figure><p>注意这里就是地址3的bin，我就不写公式格式了</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;matlab</span><br></pre></td></tr></table></figure><p>然后MATLAB就启动了！</p><h1 id="需要注意的问题"><a href="#需要注意的问题" class="headerlink" title="需要注意的问题"></a>需要注意的问题</h1><h2 id="1-挂载地址"><a href="#1-挂载地址" class="headerlink" title="1. 挂载地址"></a>1. 挂载地址</h2><p>挂载地址还是放在固态硬盘上，放在机械硬盘上很容易宕机。</p><h2 id="2-mount"><a href="#2-mount" class="headerlink" title="2. mount"></a>2. mount</h2><p>与挂载相关的很多问题其实都是因为你没有进入管理员模式，请立刻进入管理员模式，否则采取贪心方法出现一个问题解决一个问题只会越来越跑偏，如果发现一些和权限有关的问题请及时纠偏。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>SoftwareEngineering</title>
      <link href="2021/01/11/SoftwareEngineering/"/>
      <url>2021/01/11/SoftwareEngineering/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="1-需求分析"><a href="#1-需求分析" class="headerlink" title="1.需求分析"></a>1.需求分析</h1><p>目前的政治事件语料存在提取出来的事件是无关事件而导致的质量参差不齐的状况。如一篇常见的新闻内容，会在内容中简明扼要的讲述主要事件，然后引述多人对于该事件评论或者讲述一些相关事件，那么目前的语料系统会很大可能将该新闻内容归纳为评论类或者别的类别的事件。</p><p>综上，需要对于政治事件语料进行筛选，选出质量更高的事件语料。</p><h1 id="2-解决思路"><a href="#2-解决思路" class="headerlink" title="2.解决思路"></a>2.解决思路</h1><p>现有的语料中，确定的eventtext即触发词需要判断其是否有效，本章节主要为判断其触发词的有效性从而来获取判断语料的质量高低。前文已经选择TextRank可以对于文章的关键词进行提取，则我们需要通过触发词和TextRank的关键词进行匹配，匹配的结果决定了判断政治事件语料质量高低的结果。</p><p>TextRank相当于是给予文章中的每一个词进行打分，采用分高的选取为关键词。所以我们最终采用的方式是根据句子的长度来匹配TextRank的长度。可以很好的解决字数过长而导致的评分相对过高的问题和语料中少量的事件文章重复的问题。如1000字生成11个关键词，每多100字增加1个关键词。（因为eventtext的个数和文章的多少有关。）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_textrank</span>(<span class="params">txt</span>):</span></span><br><span class="line">    tr4w = TextRank4Keyword()</span><br><span class="line">    tr4w.analyze(text=txt, lower=<span class="literal">True</span>, window=<span class="number">2</span>)  <span class="comment"># py2中text必须是utf8编码的str或者unicode对象，py3中必须是utf8编码的bytes或者str对象</span></span><br><span class="line">    len_textrank = round(len(txt) / <span class="number">1000</span> * <span class="number">10</span>) + <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> tr4w.get_keywords(len_textrank, word_min_len=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h1 id="3-具体解决步骤"><a href="#3-具体解决步骤" class="headerlink" title="3.具体解决步骤"></a>3.具体解决步骤</h1><h2 id="3-1数据预处理"><a href="#3-1数据预处理" class="headerlink" title="3.1数据预处理"></a>3.1数据预处理</h2><p>首先我们循环获取每一个单独的语料，语料中包括：新闻内容，预料判断出来的content，source，target，eventtext，eventroot。其中新闻的内容由于是人民日报的很规范的内容，每个新闻的第一句是这篇新闻的标题。首先根据连续的三个“NULL”将新闻截取出来，将新闻通过空格截断从而获取第一句话即新闻标题。从而获取了所有的语料需要获取的内容。</p><h2 id="3-2思路一（动词字典）"><a href="#3-2思路一（动词字典）" class="headerlink" title="3.2思路一（动词字典）"></a>3.2思路一（动词字典）</h2><p>本思路为通过动词词典即eventroot的汇总进行对于词汇的匹配，即对于同一种eventroot类型，我们有这一类型的不同的触发词，若触发词和关键词属于同一个类型，则认为匹配成功。</p><h3 id="3-2-1预处理动词字典"><a href="#3-2-1预处理动词字典" class="headerlink" title="3.2.1预处理动词字典"></a>3.2.1预处理动词字典</h3><p>由于动词词典是.txt文件，且在归类中还存在同一个词汇从属于不同类别的情况，需要对于动词词典进行预处理。代码详见<strong>create_dictionary.py</strong>。</p><h3 id="3-2-2使用动词字典"><a href="#3-2-2使用动词字典" class="headerlink" title="3.2.2使用动词字典"></a>3.2.2使用动词字典</h3><p>在将动词词典（约1200个动词）变成csv文件格式之后（格式为词对应着一个属于类的类别序号），如果需要某个词，对于这个词，它从属的所有的类别都将作为参考的价值，所以可以从这个词向量中获取调用这个词汇属于所有类别的集合。</p><h2 id="3-3思路二（词向量模型）"><a href="#3-3思路二（词向量模型）" class="headerlink" title="3.3思路二（词向量模型）"></a>3.3思路二（词向量模型）</h2><p>本思路的出发为对于词语的匹配，需要有相似度的匹配，如果采用字词作为标准则会出现只存在有或无，0或1的问题，那么将不利于更加细致地处理匹配结果，所以决定采用通过词向量模型辅助词语的匹配。</p><h3 id="3-3-1预训练词向量模型"><a href="#3-3-1预训练词向量模型" class="headerlink" title="3.3.1预训练词向量模型"></a>3.3.1预训练词向量模型</h3><p>本次词向量模型主要通过word2vec的方法来对于所有词汇赋予其独有的词向量。word2vec这种方法是当今相对比较热门的算法，它通过神经网络机器学习算法来训练N-gram语言模型，并在训练过程中求出word所对应的vector的方法。</p><p>所以对于这个思路，首先，我们需要把所有的政治事件语料进行合并，也就是我们训练模型需要将所有的语料内容（约35000篇文章）放入训练。但对于训练word2vec词向量而言，其实这个数据量相对还是小了一些。</p><p>针对所有的文章，首先采用jieba分词，对于文章进行分割（考虑到句子之间的关系有一部分体现在关系介词等联系上下文语句的词汇和一些基本词汇上，我们不删除停用词，）然后将所有的文章投喂到word2vec后，形成一个庞大的模型，这个模型中有对于每个词汇的独有的词向量。</p><p>word2vec学习资料：<a href="https://zhuanlan.zhihu.com/p/56343249">https://zhuanlan.zhihu.com/p/56343249</a></p><h3 id="3-3-2使用词向量模型"><a href="#3-3-2使用词向量模型" class="headerlink" title="3.3.2使用词向量模型"></a>3.3.2使用词向量模型</h3><p>word2vec生成的模型可以通过调用相似度函数来测试两个词汇之间的相似度，表示两个词汇之间的关系，判断其相似度。相似度的分布区间为0到1,0代表完全不相似，1代表完全相同，相似度越高，代表这两个词相关性越高。或者可以测试一些词之间的更高级的关系。如：Embedding(“皇帝”)-Embedding(“男”)=Embedding(“皇后”)-Embedding(“女”)。</p><h2 id="3-4统一流程"><a href="#3-4统一流程" class="headerlink" title="3.4统一流程"></a>3.4统一流程</h2><p>首先针对每一篇文章，获取文章内容和eventtext和eventroot。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_information</span>(<span class="params">file_in</span>):</span></span><br><span class="line">    text_in = open(file_in, <span class="string">&#x27;rb&#x27;</span>).read()</span><br><span class="line">    <span class="comment"># 文章截取出来</span></span><br><span class="line">    clean_text = text_in.split(<span class="string">b&#x27;NULL&#x27;</span>, <span class="number">-1</span>)</span><br><span class="line">    keywords = clean_text[<span class="number">8</span>]</span><br><span class="line">    clean_keywords = keywords.split(<span class="string">b&#x27;\n&#x27;</span>, <span class="number">-1</span>)</span><br><span class="line">    size_clean_keywords = int((len(clean_keywords) - <span class="number">11</span>) / <span class="number">9</span> + <span class="number">1</span>)</span><br><span class="line">    eventtext_in = []</span><br><span class="line">    source_in = []</span><br><span class="line">    target_in = []</span><br><span class="line">    event_root_in = []</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(size_clean_keywords):</span><br><span class="line">        eventtext_in.append(clean_keywords[<span class="number">8</span> + j * <span class="number">9</span>].replace(bytes(<span class="string">&#x27;eventtext\t&#x27;</span>.encode(<span class="string">&#x27;utf-8&#x27;</span>)), bytes(<span class="string">&#x27;&#x27;</span>.encode(<span class="string">&#x27;utf-8&#x27;</span>))))</span><br><span class="line">        source_in.append(clean_keywords[<span class="number">6</span> + j * <span class="number">9</span>].replace(bytes(<span class="string">&#x27;Source\t&#x27;</span>.encode(<span class="string">&#x27;utf-8&#x27;</span>)), bytes(<span class="string">&#x27;&#x27;</span>.encode(<span class="string">&#x27;utf-8&#x27;</span>))))</span><br><span class="line">        target_in.append(clean_keywords[<span class="number">7</span> + j * <span class="number">9</span>].replace(bytes(<span class="string">&#x27;Target\t&#x27;</span>.encode(<span class="string">&#x27;utf-8&#x27;</span>)), bytes(<span class="string">&#x27;&#x27;</span>.encode(<span class="string">&#x27;utf-8&#x27;</span>))))</span><br><span class="line">        event_root_in.append(clean_keywords[<span class="number">9</span> + j * <span class="number">9</span>].replace(bytes(<span class="string">&#x27;eventroot\t&#x27;</span>.encode(<span class="string">&#x27;utf-8&#x27;</span>)), bytes(<span class="string">&#x27;&#x27;</span>.encode(<span class="string">&#x27;utf-8&#x27;</span>))))</span><br><span class="line">    article = clean_text[<span class="number">6</span>]</span><br><span class="line">    result_in = clean_text[<span class="number">8</span>]</span><br><span class="line">    text = article.decode(<span class="string">&quot;utf-8&quot;</span>).split(<span class="string">&#x27;\u3000\u3000&#x27;</span>)</span><br><span class="line">    title_in = text[<span class="number">1</span>]</span><br><span class="line">    <span class="comment"># print(article.decode(&quot;utf8&quot;))</span></span><br><span class="line">    <span class="comment"># text = text.decode(&quot;utf8&quot;).split(&#x27; &#x27;)</span></span><br><span class="line">    <span class="comment"># print(article.decode(&quot;utf8&quot;))</span></span><br><span class="line">    <span class="comment"># print(article.decode(&quot;utf8&quot;).strip(&#x27;\t&#x27;).strip(&#x27;\r&#x27;).replace(&#x27;\u3000\u3000&#x27;, &#x27; &#x27;))</span></span><br><span class="line">    clean_article_in = article.decode(<span class="string">&quot;utf-8&quot;</span>).strip(<span class="string">&#x27;\t&#x27;</span>).strip(<span class="string">&#x27;\r&#x27;</span>).replace(<span class="string">&#x27;\u3000\u3000&#x27;</span>, <span class="string">&#x27; &#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> clean_article_in, article, title_in, result_in, eventtext_in, source_in, target_in, event_root_in</span><br></pre></td></tr></table></figure><p>根据文章大小获取TextRank的关键词，词数的多少与文章大小有关。TextRank中每个关键词都有一个相应的匹配的权重，权重为TextRank所生成的，权重越高，则代表越接近最佳判断的关键词。</p><p>TextRank个人学习博客：<a href="https://shijiasheng.top/2020/11/05/TextRank/">https://shijiasheng.top/2020/11/05/TextRank/</a></p><h3 id="3-4-1动词词典"><a href="#3-4-1动词词典" class="headerlink" title="3.4.1动词词典"></a>3.4.1动词词典</h3><p>设$score$为最终得分，设$word$为TextRank关键字和event text触发词匹配上的词，设$weight$为$word$根据TextRank获得的相对于句子的权重。设$N$为匹配成功的个数，</p><p>$score=\sum_{i=0}^N weight(i)/N$</p><p>设最终得分为score=0。</p><p>将TextRank关键字和eventtext逐一比对，两两匹配，若匹配成功得分，score加上TextRank所求的关键字所对应的权重。</p><p>若这两个匹配的词汇中任意一个词不存在词向量模型中（即动词词典中没有记录该词），则匹配两个词汇是否在文字的字符串上匹配（匹配条件为，一个字符串是另一个字符串的子集）。</p><p>最终获取的score除以eventtext触发词的数量（相当于取平均值），为最终的结果。</p><p>（注：最终获取的score除以eventtext的数量的目的在于通过平均值的方式降低因为文章长度而匹配次数更高，从而分数更高的情况）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_score2</span>(<span class="params">eventtext, article_textrank, title_textrank, reader_words</span>):</span></span><br><span class="line">    score = <span class="number">0.00000000000</span></span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    score = float(score)</span><br><span class="line">    event_text_count = len(eventtext)</span><br><span class="line">    clean_eventtext = []</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> title_textrank:</span><br><span class="line">        <span class="keyword">for</span> et <span class="keyword">in</span> eventtext:</span><br><span class="line">            et = et.decode(<span class="string">&quot;utf-8&quot;</span>)</span><br><span class="line">            et = et.strip(<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">            et = et.strip(<span class="string">&#x27;\r&#x27;</span>)</span><br><span class="line">            et = str(et)</span><br><span class="line">            <span class="keyword">if</span> item.word <span class="keyword">in</span> reader_words <span class="keyword">and</span> et <span class="keyword">in</span> reader_words:</span><br><span class="line">                item_word_number = []</span><br><span class="line">                et_number = []</span><br><span class="line">                <span class="keyword">for</span> row <span class="keyword">in</span> worddic:</span><br><span class="line">                    <span class="keyword">if</span> row[<span class="string">&#x27;name&#x27;</span>] == item.word:</span><br><span class="line">                        item_word_number.append(row[<span class="string">&#x27;number&#x27;</span>])</span><br><span class="line">                    <span class="keyword">if</span> row[<span class="string">&#x27;name&#x27;</span>] == et:</span><br><span class="line">                        et_number.append(row[<span class="string">&#x27;number&#x27;</span>])</span><br><span class="line">                <span class="comment"># print(item_word_number)</span></span><br><span class="line">                <span class="comment"># print(et_number)</span></span><br><span class="line">                <span class="keyword">if</span> set(item_word_number).issubset(set(et_number)) <span class="keyword">or</span> set(et_number).issubset(set(item_word_number)):</span><br><span class="line">                    <span class="comment"># print(&quot;TRUE\n&quot;)</span></span><br><span class="line">                    score = score + item.weight</span><br><span class="line">                    count = count + <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> item.word == et:</span><br><span class="line">                    score = score + item.weight</span><br><span class="line">                    count = count + <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> article_textrank:</span><br><span class="line">        <span class="keyword">for</span> et <span class="keyword">in</span> eventtext:</span><br><span class="line">            et = et.decode(<span class="string">&quot;utf-8&quot;</span>)</span><br><span class="line">            et = et.strip(<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">            et = et.strip(<span class="string">&#x27;\r&#x27;</span>)</span><br><span class="line">            et = str(et)</span><br><span class="line">            <span class="keyword">if</span> item.word <span class="keyword">in</span> model <span class="keyword">and</span> et <span class="keyword">in</span> model:</span><br><span class="line">                similarity = model.similarity(item.word, et)</span><br><span class="line">                <span class="keyword">if</span> similarity &gt;= <span class="number">0.5</span>:</span><br><span class="line">                    score = score + item.weight * similarity</span><br><span class="line">                    count = count + <span class="number">1</span></span><br><span class="line">                    clean_eventtext.append(et)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> item.word == et:</span><br><span class="line">                    score = score + item.weight</span><br><span class="line">                    count = count + <span class="number">1</span></span><br><span class="line">                    clean_eventtext.append(et)</span><br><span class="line">    <span class="keyword">if</span> event_text_count == <span class="number">0</span>:</span><br><span class="line">        score = <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        score = score / event_text_count</span><br><span class="line">    clean_eventtext = np.unique(clean_eventtext)</span><br><span class="line">    <span class="keyword">return</span> score, event_text_count, count, clean_eventtext</span><br></pre></td></tr></table></figure><h3 id="3-4-2词向量模型"><a href="#3-4-2词向量模型" class="headerlink" title="3.4.2词向量模型"></a>3.4.2词向量模型</h3><p><strong>词向量模型公式：</strong></p><p>设$score$为最终得分，设$word$为TextRank关键字和event text触发词匹配上的词，设$weight$为$word$根据TextRank获得的相对于句子的权重，设$similarity$为TextRank关键字和event text触发词匹配的相似度，设$N$为匹配成功的个数。</p><p>$score=\sum_{i=0}^N weight(i)*similarity(i)/N$</p><p><strong>具体步骤：</strong></p><p>设最终得分为score=0。</p><p>将TextRank关键字和eventtext逐一比对，两两匹配，若匹配成功得分，score加上TextRank所求的关键字所对应的权重*两个词汇的相似度。</p><p>（注：经过最后测试，设定阈值为0.5，当两两匹配的相似度大于0.5时，认为匹配成功。）</p><p>若这两个匹配的词汇中任意一个词不存在词向量模型中（即模型中没有记录该词汇对应的词向量），则匹配两个词汇是否在文字的字符串上匹配（匹配条件为，一个字符串是另一个字符串的子集）。</p><p>最终获取的score除以eventtext触发词的数量（相当于取平均值），为最终的结果。</p><p>（注：最终获取的score除以eventtext的数量的目的在于通过平均值的方式降低因为文章长度而匹配次数更高，从而分数更高的情况）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_score</span>(<span class="params">eventtext, article_textrank, title_textrank</span>):</span></span><br><span class="line">    score = <span class="number">0.00000000000</span></span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    score = float(score)</span><br><span class="line">    event_text_count = len(eventtext)</span><br><span class="line">    clean_eventtext = []</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> title_textrank:</span><br><span class="line">        <span class="keyword">for</span> et <span class="keyword">in</span> eventtext:</span><br><span class="line">            et = et.decode(<span class="string">&quot;utf-8&quot;</span>)</span><br><span class="line">            et = et.strip(<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">            et = et.strip(<span class="string">&#x27;\r&#x27;</span>)</span><br><span class="line">            et = str(et)</span><br><span class="line">            <span class="keyword">if</span> item.word <span class="keyword">in</span> model <span class="keyword">and</span> et <span class="keyword">in</span> model:</span><br><span class="line">                similarity = model.similarity(item.word, et)</span><br><span class="line">                <span class="keyword">if</span> similarity &gt;= <span class="number">0.5</span>:</span><br><span class="line">                    score = score + item.weight * similarity</span><br><span class="line">                    count = count + <span class="number">1</span></span><br><span class="line">                    clean_eventtext.append(et)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> item.word == et:</span><br><span class="line">                    score = score + item.weight</span><br><span class="line">                    count = count + <span class="number">1</span></span><br><span class="line">                    clean_eventtext.append(et)</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> article_textrank:</span><br><span class="line">        <span class="keyword">for</span> et <span class="keyword">in</span> eventtext:</span><br><span class="line">            et = et.decode(<span class="string">&quot;utf-8&quot;</span>)</span><br><span class="line">            et = et.strip(<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">            et = et.strip(<span class="string">&#x27;\r&#x27;</span>)</span><br><span class="line">            et = str(et)</span><br><span class="line">            <span class="keyword">if</span> item.word <span class="keyword">in</span> model <span class="keyword">and</span> et <span class="keyword">in</span> model:</span><br><span class="line">                similarity = model.similarity(item.word, et)</span><br><span class="line">                <span class="keyword">if</span> similarity &gt;= <span class="number">0.5</span>:</span><br><span class="line">                    score = score + item.weight * similarity</span><br><span class="line">                    count = count + <span class="number">1</span></span><br><span class="line">                    clean_eventtext.append(et)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> item.word == et:</span><br><span class="line">                    score = score + item.weight</span><br><span class="line">                    count = count + <span class="number">1</span></span><br><span class="line">                    clean_eventtext.append(et)</span><br><span class="line">    <span class="keyword">if</span> event_text_count == <span class="number">0</span>:</span><br><span class="line">        score = <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        score = score / event_text_count</span><br><span class="line">    clean_eventtext = np.unique(clean_eventtext)</span><br><span class="line">    <span class="keyword">return</span> score, event_text_count, count, clean_eventtext</span><br></pre></td></tr></table></figure><p>若平均值为0则说明没有匹配上关键词，得分大小反映了关键词获取的质量。</p><p>最终以字典形式输出。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">form_dictionary</span>(<span class="params">name, score, event_text_count, article_textrank, count, eventtext, title_textrank, event_root, title, in_text</span>):</span></span><br><span class="line">    dict = &#123;&#125;</span><br><span class="line">    dict[<span class="string">&#x27;name&#x27;</span>] = str(name)</span><br><span class="line">    dict[<span class="string">&#x27;score&#x27;</span>] = str(score)</span><br><span class="line">    dict[<span class="string">&#x27;event_text_count&#x27;</span>] = str(event_text_count)</span><br><span class="line">    dict[<span class="string">&#x27;textrank_count&#x27;</span>] = str(len(article_textrank))</span><br><span class="line">    dict[<span class="string">&#x27;count&#x27;</span>] = str(count)</span><br><span class="line"></span><br><span class="line">    dict[<span class="string">&#x27;event_text&#x27;</span>] = str(eventtext)</span><br><span class="line">    textrank = []</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> article_textrank:</span><br><span class="line">        textrank.append(item.word)</span><br><span class="line">    dict[<span class="string">&#x27;textrank&#x27;</span>] = str(textrank)</span><br><span class="line">    title_textrank_new = []</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> title_textrank:</span><br><span class="line">        title_textrank_new.append(item.word)</span><br><span class="line">    dict[<span class="string">&#x27;title_textrank&#x27;</span>] = str(title_textrank_new)</span><br><span class="line">    event_root_new = []</span><br><span class="line">    <span class="keyword">for</span> er <span class="keyword">in</span> event_root:</span><br><span class="line">        et = er.decode(<span class="string">&quot;utf-8&quot;</span>)</span><br><span class="line">        et = et.strip(<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">        et = et.strip(<span class="string">&#x27;\r&#x27;</span>)</span><br><span class="line">        et = str(et)</span><br><span class="line">        event_root_new.append(et)</span><br><span class="line">    dict[<span class="string">&#x27;event_root&#x27;</span>] = str(event_root_new)</span><br><span class="line"></span><br><span class="line">    dict[<span class="string">&#x27;title&#x27;</span>] = str(title)</span><br><span class="line">    dict[<span class="string">&#x27;text&#x27;</span>] = str(in_text.decode(<span class="string">&quot;utf-8&quot;</span>))</span><br><span class="line">    <span class="keyword">return</span> dict, event_root_new</span><br></pre></td></tr></table></figure><p>输出格式为：</p><p>name：文章名字（编号）</p><p>score：最终质量的评分（得分）</p><p>event_text_count：文章的eventtext的数量</p><p>count：匹配上了的次数的数量</p><p>textrank_count： textrank的数量</p><p>event_root： event_root</p><p>textrank：生成的textrank</p><p>title_textrank：题目的textrank</p><p>title：文章的题目</p><p>text：文章内容（包括题目）</p><p><img src="https://i.loli.net/2020/11/24/gZFsKjOfh8meMaX.png" alt="image-20201124190638749"></p><h1 id="4-伪代码"><a href="#4-伪代码" class="headerlink" title="4.伪代码"></a>4.伪代码</h1> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">SCREEN THE EVENT WITH HIGH QUALITY(event, eventtext)</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;1. model:</span><br><span class="line">def similarity(x,y)</span><br><span class="line">x&#x3D;model.GetEmbedding(x)</span><br><span class="line">y&#x3D;model.GetEmbedding(y)</span><br><span class="line">return model.GetSimilarity(x,y)</span><br><span class="line"></span><br><span class="line">textrank &#x3D; GetTextRank(event)</span><br><span class="line">score &#x3D; 0</span><br><span class="line">count &#x3D; sizeof(eventtext)</span><br><span class="line">if similarity(eventtext, textrank) &gt; threshold:</span><br><span class="line">score +&#x3D; similarity(eventtext, textrank) * textrank.value</span><br><span class="line">score &#x3D; socre&#x2F;count</span><br><span class="line">return score</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;2. dictionary:</span><br><span class="line">def similarity(x,y)</span><br><span class="line">If getModelType(x) belongs to getModelType(y) or getModelType(y) belongs to getModelType(x):</span><br><span class="line">return true</span><br><span class="line"></span><br><span class="line">textrank &#x3D; GetTextRank(event)</span><br><span class="line">score &#x3D; 0</span><br><span class="line">count &#x3D; sizeof(eventtext)</span><br><span class="line">If getModelType(x) belongs to getModelType(y) or getModelType(y) belongs to getModelType(x):</span><br><span class="line">score +&#x3D; textrank.value</span><br><span class="line">score &#x3D; socre&#x2F;count</span><br><span class="line">return score</span><br></pre></td></tr></table></figure><h1 id="5-优化日志"><a href="#5-优化日志" class="headerlink" title="5.优化日志"></a>5.优化日志</h1><p>1、文字匹配的方式为单纯的字和字的匹配（是否是子集），同时最终score除以的是匹配的次数。</p><p>2、升级文字匹配的方式为词向量模型的方式。</p><p>3、升级文字匹配的方式为动词词典的方式。</p><p>4、升级，最终score除以的不再是匹配的次数，而是eventtext的次数，获得更加精确的判断。</p><p>如果除以匹配的次数，则会拉低某些匹配效果较好的得分（如：存在某个匹配效果极好，但是其他词语匹配效果一般的状况，这种情况若除以匹配的次数，则会将效果好的当作效果较差的）。所以除以eventtext的次数。同时这种方法能很好地处理某些语料存在eventtext不断重复的小问题。</p><p>5、升级，将表格里增加了eventtext的count的显示。</p><p>6、升级，将表格里增加了eventroot的显示。</p><p>7、升级，将文章的标题单独出来，提取标题的关键词，在原有的基础上，增加标题的关键词的权重。</p><p>经过大量的阅读和调查，发现人民日报的行文十分规范，故语料的结构相对较为清晰，语料的第一句提取出来即为该文章的标题。由于人民日报的标题专业性强，能很好地概括事件内容。故将文章的标题单独提取出来，增加文章内容的权重。根据以上匹配文章中关键词的方法，通过TextRank将文章标题提取出三个关键词出来，和eventtext进行匹配，若匹配成功，按照前文所述方式score加分，因而增加了文章的权重。</p><p>8、升级，将表格里增加了title，title的textrank的显示。</p><p>9、升级，改为通过文章的字数来判断textrank所获取的关键字的数量。</p><p>目前设置为1000字13个关键词起步，若增加/减少100字，则相应增加/减少1个TextRank的关键词。例如1924字的语料共提取22个TextRank关键词。</p><p>10、考虑尝试将source，target加入匹配中，但由于动词词典属于动词范围，而source和target从词性角度应为名词，故对于采取动词词典方式没有特别的影响。主要影响的是通过词向量模型的方式测试的效果。</p><h1 id="6-结果汇总"><a href="#6-结果汇总" class="headerlink" title="6.结果汇总"></a>6.结果汇总</h1><h2 id="6-1-人工标记"><a href="#6-1-人工标记" class="headerlink" title="6.1 人工标记"></a>6.1 人工标记</h2><p>最初采用相对盲目的选取语料方式进行打分，发现自己的打分范围不足以概括所有的情况，效果不尽如人意。</p><p>故采取第二种方式：由于项目数量相对较大，故在获取两组数据后，按照当前情况的得分进行排序，没100个中选择一个，即从35000条数据中获取350条数据，打乱顺序，人工手动打分（相对比较严格）。打分结束重新按照得分多少恢复数据。从而判断阈值。</p><h2 id="6-2-数据匹配与阈值筛选"><a href="#6-2-数据匹配与阈值筛选" class="headerlink" title="6.2 数据匹配与阈值筛选"></a>6.2 数据匹配与阈值筛选</h2><p>根据结果显示：</p><p>词向量模型的方法score不为0个数（匹配成功）为33247个。</p><p>动词词典的方法score不为0个数（匹配成功）为25990个。</p><p>比较了词向量模型的方式和动词字典的方式这两种方式，发现词向量模型的方式匹配的结果更加符合，相对而言动词词典存在不完整的情况，效果相对于词向量模型稍微差一些。</p><p>根据研究发现，以上现象的主要原因是动词词典的不完整，需要或者说可以进一步完善。</p><p>根据人工手动打分的比对，两者的结点的score取值的阈值为：</p><p>词向量模型：0.024</p><p>动词词典：0.0013</p><h2 id="6-3-两种方法结合"><a href="#6-3-两种方法结合" class="headerlink" title="6.3 两种方法结合"></a>6.3 两种方法结合</h2><p>故采取综合两种方法的方式，一种为两者得分相加，一种为两者求完阈值之后合并。</p><p>我对于两种方法皆进行了测试。</p><p>按照两者求完阈值之后合并，获得了24391条筛选之后的语料。于<strong>合并结果.txt</strong>中</p><p>两者相加得分的阈值为：0.03 ，共29967个。</p><h2 id="6-4-筛选结果"><a href="#6-4-筛选结果" class="headerlink" title="6.4 筛选结果"></a>6.4 筛选结果</h2><p>筛掉掉的语料中存在着不少触发词提取不准确的现象：</p><h2 id="6-4-1-新闻事件中评论过多，被认为是评论类新闻，实则不然。"><a href="#6-4-1-新闻事件中评论过多，被认为是评论类新闻，实则不然。" class="headerlink" title="6.4.1 新闻事件中评论过多，被认为是评论类新闻，实则不然。"></a>6.4.1 新闻事件中评论过多，被认为是评论类新闻，实则不然。</h2><p>举例：</p><p><strong>[‘010 作出评论 ‘]：</strong>载231人客机波兰迫降无人伤亡(组图)　　中新网11月2日电 据外电报道，当地时间1日，一架载有231人、从美国飞往波兰的客机由于发生机械故障，紧急迫降，所幸机上无人受伤。两架F-16战机升空护送客机。　　据报道，该架从美国纽瓦克机场飞往波兰华沙的波音767-35D型客机在准备着陆时起落架发生故障，无法正常打开，不得不在机场上空盘旋了1个小时，以释放掉客机多余的燃料，最后靠机身腹部迫降。　　一名波兰空军发言人说，得知情况后，空军方面派出两架F-1 6战斗机，升空护送客机，并提供相应协助。机场方面暂停所有航班起降，并在跑道上铺设了帮助迫降的材料。　　波兰TVN24新闻频道的画面显示，客机使用机身腹部迫降到地面，机腹部位出现一团白色泡雾状物，客机随后在跑道上滑行。　　据悉，该架客机上搭载着220名乘客和11名机组人员，事件没有造成人员受伤。　　客机降落后，乘客迅速撤离，消防车立即向客机洒水降温。该客机所属的波兰航空公司称，飞机之所以能够成功迫降，完全凭借飞行员高超的驾驶技术。据介绍，该名飞行员对波音客机有20年的飞行经验。　　波兰总统科莫罗夫斯基(Bronislaw Komorowski)对这架客机的飞行员及其它机组成员表示感谢，称赞他们在“危难时刻有效合作”。    </p><p><strong>[‘010 作出评论 ‘]：</strong>利比亚过渡委发现卡扎菲在苏尔特地下城堡(图)　　环球网实习记者赵小侠报道，据英国《每日电讯报》10月19日报道，利比亚“国家过渡委员会”在苏尔特一座被遗弃的住宅下发现了卡扎菲的地下城堡，利比亚当局官员表示，卡扎菲很有可能还在苏尔特城中。　　据报道，就像苏尔特的其他地方一样，卡扎菲昔日的豪宅现在已经成了一片废墟，豪华的装饰品遭到破坏，房间内现在满是瓦砾。利比亚“过渡委”士兵将记者带领至这片废墟下的“地下迷宫”，在这里，记者看到床垫还铺在地面上，这就是说最近这座地堡还有人住过。　　此前，“过渡委”指挥官曾表示，他85%的肯定卡扎菲、卡扎菲儿子穆塔西姆和其他卡扎菲政权高官仍藏匿于苏尔特。　　据悉，自从利比亚当局攻陷拜尼沃利德之后，苏尔特成了卡扎菲支持者占据的最后一座城市。“过渡委”最近正式宣布，只有苏尔特得以完全解放，才会为成立完善的政府、创造一个民主国家铺平道路。    </p><h2 id="6-4-2-触发词提取不准确，不具备独特性。"><a href="#6-4-2-触发词提取不准确，不具备独特性。" class="headerlink" title="6.4.2 触发词提取不准确，不具备独特性。"></a>6.4.2 触发词提取不准确，不具备独特性。</h2><p>举例：</p><p><strong>[‘064 共享信息 ‘]：</strong>巴基斯坦海军宣传照错印印度战舰(图)　　环球网记者仲伟东报道，《印度时报》3月9日消息称，巴基斯坦海军8日公布的多国海军联合军演宣传照“出岔子”，并未参加该次演习的印度海军军舰的形象竟然出现在宣传照上，“让人啼笑皆非”。　　报道说，这幅宣传照印有“携手为和平”的宣传口号，旨在宣传代号“阿曼-11”的联合演习，照片上还印有印度海军的“戈达瓦里”级和“塔尔瓦”级战舰的图像。报道指出，该宣传照上的美印军舰形象，可能来自2010年“马拉巴尔”美印海军联合演习时所用的宣传照。目前巴基斯坦海军暂未对此事发表评论。　　报道介绍称，将有来自39个国家的战舰、战机、特种部队和代表参加该次演习，演习旨在促进该地区和平，并打击该地区的海上威胁。　　《印度时报》同时指出，这并非印巴两国首次“摆乌龙”。去年3月，巴基斯坦旁遮普省警方公布的一幅宣传照上，竟然印有印度旁遮普警察的标志。而在去年1月份的一幅印度政府宣传照中，巴基斯坦前空军参谋长坦维尔?艾哈迈德居然和印度总理辛格站在一起。    </p><h2 id="6-4-3-由于语义的关系，没有正确识别情感取向，错将相反的情感取向提取。"><a href="#6-4-3-由于语义的关系，没有正确识别情感取向，错将相反的情感取向提取。" class="headerlink" title="6.4.3 由于语义的关系，没有正确识别情感取向，错将相反的情感取向提取。"></a>6.4.3 由于语义的关系，没有正确识别情感取向，错将相反的情感取向提取。</h2><p>举例：</p><p><strong>[‘090 期望人道主义救援 ‘]：</strong>日本核污水或将污染海洋生物链　　如今人们并不清楚福岛附近海域都有哪些鱼类，会如何迁徙。如果鱼类在污染区停留时间很长，那么通过食物链富集到鱼体内的铯可能会形成污染　　《国际先驱导报》记者蓝建中、金微发自东京、北京从5级直接升到最高的7级，当4月12日，日本经济产业省原子能安全保安院与原子能安全委员会正式宣布，将福岛第一核电站事故的严重程度“连升两级”后，对于福岛的核泄漏是否会超过切尔诺贝利的担心开始多了起来。　　由于事故仍然处于进行时，东京电力公司干部松本纯一甚至指出：“现在仍担心泄漏量有可能与切尔诺贝利相匹敌，甚至还会超越切尔诺贝利事故。”　　而最让人担心的，就是福岛核电站流出的污水。　　日本民众的愤怒和恐慌　　在周半国家的抗议和不满声中，4月10日，日本终于停止了向大海排放放射性核废水，6天时间，1.15万吨相当于4个奥运标准泳池的低浓度放射污染废水排入海中。　　这些放射性废水对大海造成什么影响尚难以估量，但恐慌和担忧情绪已在日本国内蔓延。　　东京的大型超市里，购买鲜鱼时犹豫不决的顾客越来越多。不仅如此，商家在核电站事故之前采购的咸鱼和干鱼都有些滞销，他们只能想办法促销。一些寿司店表示，自从福岛核事故以来，他们不断被顾客询问安全性问题。　　实际上，对于东电公司和日本政府的向海里排污的举动，日本民众从开始就感到震惊和愤怒。不满的声音首先从日本渔业传出，福岛县渔业协同组合联合会参事小野修司指出：“东京电力公司是在开始排放前两个小时才通知联合会的，根本没有时间向渔民进行解释。”　　“我手头要是有水的话，非得泼在东电总裁的脸上。卖不出去的鱼要他们赔！”茨城县渔民武子宽一脸无奈又愤怒地说。　　日本茨城县渔业协会宣布，从附近海域捕捞的海鱼被检测出放射性物质超标，受此影响，中国上海、广东、浙江等地的海产品交易量下降。中国官方在4月9日扩大了禁止从日本进口食品的品种和产地范围。　　东电没有说明这些废水的放射性浓度数值，只是强调说这是“低浓度的放射性污水”。按《核反应堆规制法》的相关规定，低浓度核污染污水通常指比普通海水中的辐射浓度高出100倍的污染水，但东电说的“低浓度核污染污水”显然并不是这样的准确的科学术语。　　“放射性浓度多少不知道，里面究竟含有哪些成份也不清楚，除了放射性物质，还可能存在其他东西。”复旦大学核科学与技术系副主任陈建新告诉《国际先驱导报》，日本方面提供的信息相当不透明。　　随着洋流扩散　　世界气象组织和国际原子能机构北京区域环境紧急响应中心高级工程师周斌对于核废水在海洋中的扩散作了一个形象的比喻：“好比墨水滴入一条河流，墨水的扩散是向周围不断稀释，并且向下游运动，下游浓度不断衰减。”　　进入大海的核废水会除了部分沉淀，还有一部分会随洋流运动。　　福岛附近海域是千岛寒流和黑潮相遇的地方，也是著名的渔场，两股洋流在这里相会后，继续北上，形成太平洋暖流，流到阿拉斯加和加拿大，继续南下到美国和墨西哥以后，又沿着赤道西流，最后流回马来群岛和中国台湾附近。　　从洋流的角度看，福岛核电站泄漏或排出的核辐射污染水体，主要被黑潮带动向西北和东北太平洋一侧扩散。　　法国国家科学研究院受国际原子能机构委托，根据福岛县海域的海底地形、潮流、水温、盐分浓度等，预测了放射性物质的扩散。结果发现，放射性物质最初会沿着海岸向南北扩散，到了北方的仙台湾以后，将会向东西方向扩散。正因为如此，虽然有来自马来群岛附近的强大黑潮从日本列岛附近向北流过，福岛县以南的茨城县也依然有鱼类被检测出放射性物质超标。　　“大气环流的的运动，一般２～３个星期就会绕地球一圈，而核放射性物质随大气环流扩散的周期比这更长一些。”周斌说，“在这过程中，由于路径长，以及空气阻力、降水沉降和自身衰减等多方影响，放射性物质在不断稀释，浓度就会不断减小。”　　日本方面的说法称，1.15万吨低浓度核废水排入大海后，洋流会把污染物稀释至原本的1％。　　而周斌认为，通过大气扩散模式计算，假如放射性物质在原地的浓度为１个单位，那么经过大气环流运动，扩散到我国后的浓度仅为１０的负２５次方左右。因此不用担心核废水对我国海域的影响。　　污染取决于水的流动　　在所排放的核废水中，放射性元素主要是碘和铯，由于放射性元素半衰期的时间长短不同，危害也不一样。碘-131的放射性半衰期是8天，经过10个半衰期，它的放射性会大大减少，危险可以忽略不计，因此参与洋流运动的放射性物质主要是铯。　　日本原子能研究开发机构研究员中野政尚曾对半衰期约为30年的放射性铯从茨城县海域扩散的情形进行过计算机模拟演算，结果发现放射性铯顺着海流，5年后将到达北美，10年后回到亚洲东部，30年后几乎扩散到整个太平洋。不过，他认为，即使泄漏2万吨高放射性污水，放射性活度在1年后也将稀释到每升约1贝克勒尔，10年后将不到0.1贝克勒尔，其水平不会对人体造成影响。　　切尔诺贝利事故后，国际一些机构的研究表明，受放射性污染较严重的是封闭水体，如湖泊，而不是河流和海洋。俄罗斯科学院曾比较了不同污染地区的淡水鱼肌肉组织中的铯-137含量，发现河流中的鱼受到的污染比湖泊中的鱼要轻。　　德国负责渔业环境放射污染监测的约翰·海因里希·冯·杜能研究所也发表公报说，根据切尔诺贝利核事故取得的经验，福岛核电站泄漏的放射性物质不会对鱼类等海洋生物造成长期污染。该研究所指出，切尔诺贝利核事故发生后，德国在过去25年中持续监测该事故的放射性污染物对邻近的大西洋和波罗的海鱼类的影响，发现放射性污染物在水流循环好的海域很快会被稀释。事故发生后第二年，德国有关海域就已检测不到核事故造成的铯污染。　　没有被稀释的　　由于海的容量巨大，放射物除了被海水稀释，也有相当一部分会沉降到海底。　　虽然日本方面一再强调这些污水对鱼类的影响有限，但是香港城市大学生物及化学系副教授林汉华称，很多废水会留在近岸，污染悬浮物质，当悬浮物质沉积到海底，会污染整个海床，慢慢传播到整条食物链。　　法国辐射防护与核安全研究院以海流数据等为基础进行计算机模拟演算，得出结论：以微粒形式沉淀在海底的放射性物质有可能造成长期污染。特别是铯-134的半衰期为数年，铯-137的半衰期为约３０年，有可能沉淀的日本海岸地区，需要长期进行监控。　　该机构指出，放射性物质还有可能在鱼类和贝类体内富集。这里指的是“生物富集”，比如自然界中一种有害的化学物质被草吸收，浓度很低，以吃草为生的兔子吃了这种草，因这种有害物质很难排出体外，便逐渐在它体内积累。老鹰以吃兔子为生，于是有害的化学物质便会在老鹰体内进一步积累。食物链对有害的化学物质有累积和放大的效应。　　如果是放射性的铯，在软体动物和海藻中的富集率是50倍，但是在鱼类中则会富集400倍。放射性碘则相反，在鱼类体内会富集15倍，海藻中是1万倍。　　根据放射性物质与生物种类的不同，污染的状况也多种多样。海带等被放射性碘-131污染的危险很大，但是碘-131的半衰期很短，所以有重大危险也只停留在几个月之内。　　切尔诺贝利事故后也表明，对于食物的影响主要是铯，它会随着食物链传递和富集，因此捕食性的鱼类肌肉组织中或许随着时间推移会有更多的放射物。　　英国普斯茅斯大学环境学家吉姆·史密斯认为，如今人们并不清楚福岛附近海域都有哪些鱼类，会如何迁徙。如果鱼类在污染区停留时间很长，那么通过食物链富集到鱼体内的铯可能会形成污染。　　更让人担心的　　在低浓度核污染废水进入大海之前，日本已经发生了一次核辐射污水泄漏事件。含高浓度辐射物质积水从日本福岛2号机组的裂缝渗入太平洋，导致机组附近海水中中含有超过法定限度７５０万倍的放射性碘，当地水产品也被严重污染。　　“这些从反应堆地底下渗透到海洋里的污水才是最大的担忧，这里面不单单是碘、铯放射物，还可能包括像破碎棒等物质，这些东西对人体会造成什么危害现在也说不清楚。”陈建新说。　　如今，日本开始着手转移积存在福岛第一核电站2号机组汽轮机厂房附近竖井内的高放射性污水。这些积水正被转移到该厂房内一个容量约为3000吨的空置冷凝器中。　　据估计，福岛第一核电站1号至3号机组涡轮机房地下室以及机组外隧道和竖井内，至少存有6万吨高放射性污水，这3000吨的存储容量显然是杯水车薪。不过，鉴于2号机组问题最为严重：其积水表面辐射量非常高，在此环境中工作4小时即可致人死亡。同时，竖井内的污水4月10日上午已升至距地表92厘米，若再不及时转移，有可能溢出。　　但是对于其余的高放射性污水的处理方法，日本方面尚未拿出方案。　　目前，日本福岛核电站附近海洋已被严重污染已是事实。美国国家地理网站称，福岛核电站附近的放射性污染对当地海洋动物的生存造成伤害。放射生态学家沃德·维克勒说，海洋动物的卵和幼体对辐射较为敏感，所导致的辐射暴露将改变它们的DNA。　　陈建新认为，现在断定日本核污水对中国无影响还为时尚早，“日本的近海的一些水产品，包括小鱼大鱼，如果最后流到我们的餐桌上，我们吃了这些东西显然也会被污染。”　　《国际先驱导报》法律声明：本报记者及特约撰稿人授权本报声明：本报所刊其撰写的稿件和提供的图片，未经本报许可，不得转载、摘编（有需转载者请致电至010—63073377或发邮件至<a href="mailto:&#x69;&#104;&#x6c;&#x2d;&#109;&#x61;&#x72;&#107;&#x65;&#116;&#64;&#118;&#105;&#x70;&#x2e;&#115;&#x69;&#110;&#x61;&#x2e;&#99;&#x6f;&#109;">&#x69;&#104;&#x6c;&#x2d;&#109;&#x61;&#x72;&#107;&#x65;&#116;&#64;&#118;&#105;&#x70;&#x2e;&#115;&#x69;&#110;&#x61;&#x2e;&#99;&#x6f;&#109;</a>）    </p><p><strong>[‘026 期望协商 ‘]：</strong>美刊称中期选举后新孤立主义将主导美国外交　　环球时报特约记者宋华报道 据美国《新闻周刊》10月23日发表分析文章称，中期选举后，是冷淡而不是激进主义将主导美国外交政策。文章认为，由于美国国内面临严重的经济和失业问题，选民不再关心外部世界所发生的事情，而各党候选人也投其所好，有意避开外交问题而大谈经济。而由于经济问题很难在短期内解决，因此这种新孤立主义的“冷淡”将在较长时间里影响美国外交。　　美国人对世界事务趋向冷淡　　文章称，在美国各党竞选辩论或者候选人的言论中，竞选广告里，阿富汗很少被提及，伊拉克战争已是历史，中东和平会谈进展会让人打呵欠。忘记拉丁美洲吧，俄罗斯已是一个古老的回忆，欧洲正在从美国人的视线中消失。一些候选人很负责任地记住了相关话题的发言要点，但许多候选人并没有那样作。当选民们被问及美国最为迫切的问题是什么时，只有3%的人回答说是阿富汗，60%的人称是经济和失业问题。　　一些政治家都对这种态度感到震惊。来自南卡罗莱纳州的共和党参议员林赛?格雷厄姆最近在华盛顿称：“任何候选人——茶党、共和党人、素食主义者、自由主义者、民主党人就我们应该对伊朗采取什么措施进行过认真的讨论吗？你看到一个有关阿战战略是好还是坏的电视竞选广告吗？你将永远无法知道这个国家正在打两场战争，我们所面临的威胁可能影响人类的走向。我不能理解的是，这种情况是如何发生的？”　　分析称，这种冷淡深深植根于这个移民国家：移民们前往美国是为了逃避冲突和贫穷。美国人传统上是以未来生活为导向的，不愿意承担过去的负担，他们不想从他们的任务中分心。今天，理解美国公众对全球事务更加缺乏兴趣是至关重要的，因为冷淡而不是激进主义将在可预期的未来引导或者限制、干扰美国政府的行动。　　美国领导人的挑战　　俄亥俄大学政治学教授约翰?米勒称，美国公众舆论中一直存在着某种橡皮带效应，公众关注的焦点总是会反弹至国内的关切。他根据多年的民调数据得出结论认为，公众的注意力可以被重大事件或者对美国人生命的具体、重大威胁所吸引，但当这些担心被缓解后，人们就会把关注点转回国内议题。米勒称，对于那些担心外国事务的国内外人士来说，“这种倾向可能就像是一种注意力不足症。”　　对美国领导人的挑战一直是抵制这种倾向。奥巴马是有史以来国际背景知识最丰富的美国领导人，他明白与世界接触的必要性。奥巴马就任时不仅承诺要行使全球的领导力，而且要与盟友结成伙伴、向敌人伸出手。如果正如许多人所预料的那样，奥巴马的民主党失去对众议院的控制，只在参议院保住微弱优势的话，奥巴马争取对外交倡议支持的能力将进一步被削弱。与伊朗和其它对美国持敌视态度的国家进行对话的希望可能被国会山上对立的多数派成员破坏。　　文章认为，多数情况下，美国公众的普遍冷淡是一种任性的无知。当暴怒横行而理性处于撤退的时期，没有人想听包括总统在内的政策书呆子来讲述复杂的外交事务。在竞选期间，“海外”是愤世嫉俗商人把工作岗位输出的地方或者是无能的现任官员放任这种现象发生的地方，或者两者兼有。与此同时，数百万移民或者是非法移民正来到美国，来夺占本来已经很少的工作岗位。在这一竞选季节，整个世界被分化成“他们”和“我们”，“他们”并没有发言权或者选票。　　忽视世界的危险　　文章表示，美国人忽视世界的倾向并不真的是一件政党政治事务或者是传统的左右翼分歧。自由派经济学家、前克林顿政府时期劳工部长罗伯特?赖克写道：“我们正处在新孤立主义的边缘，新孤立主义将伤害我们所有的人。”他指责右翼的茶党和左翼的工会赞成保护主义。正如赖克所说的那样，这可能“导致对自由贸易、移民、甚至世贸组织、世界银行和国际货币基金组织这样的国际机构的强烈抵制。”与此同时，美国企业研究所的保守派分析人士普雷卡则对公众对保护全球各地自由事业兴趣的下降发出哀叹。她警告称，美国从世界事务中后撤将堕落为一个清算中心，所有的事情都沦落为美元和美分。　　美国人在过去的许多时候也曾陷入这种冷淡但是情绪化的状态，这方面的记录是悲惨的。在第一次世界大战后，美国退出了国联，结果只是看到纳粹在德国的崛起。在冷战结束后，美国赢得了海湾战争，历史看上去于九十年代初期结束了，民主党人克林顿以“这是经济，笨蛋。”的口号赢得了总统选举。美国公众在这之后几乎完全没有对世界其它地方发生的巨大灾难有所触动。在对索马里进行短暂和血腥的干涉后，美国从索马里后退，放任其陷入混乱，然后很快忘记了这一切。就在“9?11”事件发生不久之前的2001年夏季，民调显示，恐怖主义在美国公众担心的事务排名最后。正如格雷厄姆参议员所说的那样，有时候，只有重大事件的发生才能使美国的注意力聚焦。    </p><h1 id="7-实验室其他学习成果"><a href="#7-实验室其他学习成果" class="headerlink" title="7.实验室其他学习成果"></a>7.实验室其他学习成果</h1><p>除了核心内容外还学习了其他相关内容，同时跑了甚至一点点实现了相关代码。如TextCNN，基本的GAN网络，Ziyun词云等等。</p><h2 id="7-1-TextCNN"><a href="#7-1-TextCNN" class="headerlink" title="7.1 TextCNN"></a>7.1 TextCNN</h2><p>TextCNN对于日后的帮助主要在于了解词向量的概念，同时了解如何将词语变成词向量，同时巩固了卷积神经网络的概念。项目采用的是清华大学开源的标题分类的数据，初步了解NLP。具体内容参见个人学习博客。</p><p>TextCNN个人学习博客：<a href="https://shijiasheng.top/2020/10/15/TextCNN/">https://shijiasheng.top/2020/10/15/TextCNN/</a></p><h2 id="7-2-Ziyun"><a href="#7-2-Ziyun" class="headerlink" title="7.2 Ziyun"></a>7.2 Ziyun</h2><p>依托于软工项目，调用库将关键词信息反映在图象上。</p><p><img src="https://i.loli.net/2020/12/22/Izxmu5YBJ6kZP1q.png" alt="image-20201222200432658"></p><h2 id="7-3-flask"><a href="#7-3-flask" class="headerlink" title="7.3 flask"></a>7.3 flask</h2><p>python的前端制作，学习相关知识。</p><h1 id="8-文件说明"><a href="#8-文件说明" class="headerlink" title="8.文件说明"></a>8.文件说明</h1><p>1、其他</p><p>1.csv：词向量模型与人工比对结果</p><p>2.csv：动词字典与人工比对结果</p><p>3.csv：合并结果与人工比对结果</p><p>测试.txt：人工测试的数据</p><p>动词字典.csv：动词字典</p><p>动词字典.txt：动词字典</p><p>测试：所有的语料重新编号</p><p>石稼晟有效人工判断.csv：手动判断</p><p>2、csv为所有文件，txt为筛选所得的编号，编号对应<strong>测试文件夹</strong>的序号</p><p>词向量模型结果.csv</p><p>词向量模型结果.txt</p><p>词向量模型结果.csv</p><p>词向量模型结果.txt</p><p>合并结果.csv</p><p>合并结果.txt</p><p>3、TextRank4ZH-master：代码的内容</p><p>TextRank4ZH-master</p><p>├─contract<br>├─example<br>├─test<br>│  └─doc<br>└─textrank4zh</p><p>NLP\TextRank4ZH-master\example\create_dictionary.py：创建动词字典代码</p><p>NLP\TextRank4ZH-master\example\create_model.py创建词向量模型代码</p><p>NLP\TextRank4ZH-master\example\word2vec.model生成的词向量模型</p><p>NLP\TextRank4ZH-master\example\final_text.py最终测试代码</p><p>NLP\TextRank4ZH-master\example\shaixuan.py筛选每100条选1条数据代码</p><p>NLP\TextRank4ZH-master\example\union.py合并测试代码</p><p>NLP\TextRank4ZH-master\test\ziyun_test.py字云代码</p><p>NLP\TextRank4ZH-master\textrank4zh\stopwords.txt停用词文档</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>TextRank</title>
      <link href="2020/11/05/TextRank/"/>
      <url>2020/11/05/TextRank/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>论文名称： <strong>TextRank: Bringing Order into Texts</strong></p><p>TextRank是一个很经典的NLP的方法，主要是用来获得一段文字的关键词。</p><p>TextRank思想非常简单：通过词之间的相邻关系构建网络，然后用<a href="https://link.zhihu.com/?target=http://www.cnblogs.com/en-heng/p/6124526.html">PageRank</a>迭代计算每个节点的rank值，排序rank值即可得到关键词。PageRank本来是用来解决网页排名的问题，网页之间的链接关系即为图的边，迭代计算公式如下：</p><p><img src="https://i.loli.net/2020/11/05/D15XNRwBeZdFjUP.png" alt="img"></p><p>其中，PR(Vi)表示结点Vi的rank值，In(Vi)表示结点Vi的前驱结点集合，Out(Vj)表示结点Vj的后继结点集合，d为damping factor用于做平滑。</p><p>网页之间的链接关系可以用图表示，那么怎么把一个句子（可以看作词的序列）构建成图呢？TextRank将某一个词与其前面的N个词、以及后面的N个词均具有图相邻关系（类似于N-gram语法模型）。具体实现：设置一个长度为N的滑动窗口，所有在这个窗口之内的词都视作词结点的相邻结点；则TextRank构建的词图为无向图。下图给出了由一个文档构建的词图（去掉了停用词并按词性做了筛选）：</p><p><img src="https://i.loli.net/2020/11/05/wr6z4e5ARTS7blp.png" alt="image-20201105153948032"></p><p>考虑到不同词对可能有不同的共现（co-occurrence），TextRank将共现作为无向图边的权值。那么，TextRank的迭代计算公式如下：</p><p><img src="https://i.loli.net/2020/11/05/VMPzyLU2gnO1SBx.png" alt="img"></p><p>可以看出，该公式仅仅比PageRank多了一个权重项Wji，用来表示两个节点之间的边连接有不同的重要程度。</p><p>在这里算是简单说明了TextRank的内在原理，以下对其关键词提取应用做进一步说明。</p><p>TextRank用于关键词提取的算法如下：</p><p>（1）把给定的文本T按照完整句子进行分割，即</p><p><img src="https://i.loli.net/2020/11/05/w9dQLKUB32cVEON.png" alt="img"></p><p>（2）对于每个句子Si属于T，进行分词和词性标注处理，并过滤掉停用词，只保留指定词性的单词，如名词、动词、形容词，即</p><p><img src="https://i.loli.net/2020/11/05/myLE6KxS4DOrAwQ.jpg" alt="img"></p><p>，其中 ti,j 是保留后的候选关键词。</p><p>（3）构建候选关键词图G = (V,E)，其中V为节点集，由（2）生成的候选关键词组成，然后采用共现关系（co-occurrence）构造任两点之间的边，两个节点之间存在边仅当它们对应的词汇在长度为K的窗口中共现，K表示窗口大小，即最多共现K个单词。</p><p>（4）根据上面公式，迭代传播各节点的权重，直至收敛。</p><p>（5）对节点权重进行倒序排序，从而得到最重要的T个单词，作为候选关键词。</p><p>（6）由5得到最重要的T个单词，在原始文本中进行标记，若形成相邻词组，则组合成多词关键词。</p><p>2.1 TextRank算法提取关键词短语</p><p>　　提取关键词短语的方法基于关键词提取，可以简单认为：如果提取出的若干关键词在文本中相邻，那么构成一个被提取的关键短语。</p><p>2.2 TextRank生成摘要</p><p>　　将文本中的每个句子分别看做一个节点，如果两个句子有相似性，那么认为这两个句子对应的节点之间存在一条无向有权边。考察句子相似度的方法是下面这个公式：</p><p><img src="https://i.loli.net/2020/11/05/KqcCLrEXMFuO7fk.png" alt="img"></p><p>公式中，Si,Sj分别表示两个句子词的个数总数，Wk表示句子中的词，那么分子部分的意思是同时出现在两个句子中的同一个词的个数，分母是对句子中词的个数求对数之和。分母这样设计可以遏制较长的句子在相似度计算上的优势。</p><p>我们可以根据以上相似度公式循环计算任意两个节点之间的相似度，根据阈值去掉两个节点之间相似度较低的边连接，构建出节点连接图，然后计算TextRank值，最后对所有TextRank值排序，选出TextRank值最高的几个节点对应的句子作为摘要。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>KSVD</title>
      <link href="2020/10/31/KSVD/"/>
      <url>2020/10/31/KSVD/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>如果你认出了到这篇博客的封面，说明你很有眼光哦！（请耐心阅读，文末给出答案）。</p><p>本博客代码为KSVD的MATLAB代码，增加了详细的注释，尽可能地每个函数都写了注释（基本上每行都有注释）。</p><p>地址：<a href="https://github.com/sjspingfandeshijie/KSVD">https://github.com/sjspingfandeshijie/KSVD</a></p><h1 id="KSVD算法介绍"><a href="#KSVD算法介绍" class="headerlink" title="KSVD算法介绍"></a>KSVD算法介绍</h1><p>KSVD算法，顾名思义，就是K个SVD算法，为什么要用SVD算法呢，稍后会讲。</p><p>KSVD算法本质上是字典的学习。这里的字典其实可以理解为一个翻译的工具，当我们拥有一个原始样本，我们用Y来表示，那么我们需要尽可能的用稀疏矩阵X来表示Y，也就是这个翻译的过程，用到了我们的字典Dictionary。对于字典Dictionary来说，它的每列，我们定义为原子（atom）。</p><p>所以如果我们需要一个Y的稀疏表示，则KSVD是一个功能比较强大的方法。所以这个方法时常用在图像去噪当中。</p><p>我们目前知道了原始样本Y，为了获取Y的稀疏表示，需要人为构建D和X，D为字典Dictionary，X为稀疏表示，理想的情况是Y=D * X，但是一般来说做不到，所以需要尽可能保证Y和D * X接近，同时需要满足X足够稀疏。数学表达式如下：</p><p><img src="https://i.loli.net/2020/10/31/9pC6ROvYcsAumJW.png" alt="image-20201031173854731"></p><p>或者</p><p><img src="https://i.loli.net/2020/10/31/EJQrgjYwk8z6nsx.png" alt="image-20201031174410655"></p><p>我们知道电脑只能每次实现一个功能，如何尽可能同时满足这两个要求呢？那么需要不断地迭代，在迭代过程中不断趋近于最优解。在每次迭代中都需要完成两个步骤：一个是稀疏编码，一个是字典更新。</p><p>首先是Y=D * X，Y是已知的矩阵，D是字典dictionary，X是未知的系数矩阵。</p><p>稀疏编码环节中要将系数矩阵尽可能地稀疏，而字典学习环节中则为在系数矩阵已知的情况下努力将Dictionary学习出来，使得Dictionary接近最理想的状态（Y-D*X尽可能小）。即前文的数学表达。</p><h1 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h1><h2 id="稀疏编码"><a href="#稀疏编码" class="headerlink" title="稀疏编码"></a>稀疏编码</h2><p>稀疏编码（Sparse Coding Stage）：使用OMP算法，进行对于系数矩阵的构建，满足字典足够满足要求，同时x足够稀疏。</p><p>Use any pursuit algorithm to compute the representation vectors alphaij for each path RijX, by approximating the solution of</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">% find the coefficients 发现系数</span></span><br><span class="line"><span class="keyword">if</span> (param.errorFlag==<span class="number">0</span>)  <span class="comment">%param.errorFlag = 1;   </span></span><br><span class="line">    CoefMatrix = OMP([FixedDictionaryElement,Dictionary],Data, param.L); </span><br><span class="line">    <span class="comment">%size(Data,2)=249*249</span></span><br><span class="line"><span class="keyword">else</span>  </span><br><span class="line">    <span class="comment">%param.errorGoal = sigma*C;</span></span><br><span class="line">    CoefMatrix = OMPerr([FixedDictionaryElement,Dictionary],Data, param.errorGoal);</span><br><span class="line">    <span class="comment">%param.errorGoal = sigma*C;   稀疏矩阵</span></span><br><span class="line">    param.L = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><h2 id="字典更新"><a href="#字典更新" class="headerlink" title="字典更新"></a>字典更新</h2><p>字典更新（Dictionary Update Stage）：</p><p>For each column in D, update it by…</p><p>Find the set of patches that use this atom        </p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> <span class="built_in">j</span> = rPerm</span><br><span class="line">    <span class="comment">% 初步估计是每次只更新一列</span></span><br><span class="line">    [betterDictionaryElement,CoefMatrix,addedNewVector] = I_findBetterDictionaryElement(Data,...</span><br><span class="line">        [FixedDictionaryElement,Dictionary],<span class="built_in">j</span>+<span class="built_in">size</span>(FixedDictionaryElement,<span class="number">2</span>),...</span><br><span class="line">        CoefMatrix ,param.L);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">% 只更新了第j列</span></span><br><span class="line">    Dictionary(:,<span class="built_in">j</span>) = betterDictionaryElement;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">% param.preserveDCAtom暂时=0，作用有待考证，这段代码不运行</span></span><br><span class="line">    <span class="keyword">if</span> (param.preserveDCAtom)</span><br><span class="line">        tmpCoef = FixedDictionaryElement\betterDictionaryElement;</span><br><span class="line">        Dictionary(:,<span class="built_in">j</span>) = betterDictionaryElement - FixedDictionaryElement*tmpCoef;</span><br><span class="line">        Dictionary(:,<span class="built_in">j</span>) = Dictionary(:,<span class="built_in">j</span>)./<span class="built_in">sqrt</span>(Dictionary(:,<span class="built_in">j</span>)&#x27;*Dictionary(:,<span class="built_in">j</span>));<span class="comment">%去掉一列以后重新归一化</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">% 简单的计数器</span></span><br><span class="line">    replacedVectorCounter = replacedVectorCounter+addedNewVector;<span class="comment">%实验证明（针对w.jpg图像），值累加了一次</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p>其中，调用了I_findBetterDictionaryElement函数，这个函数可以理解为一列一列得更新字典。</p><p>先到这里了，最近《数码宝贝》20周年大电影上线咯！正如封面所示，我惊奇地发现电影里数码宝贝和数码世界都是用python写的，大家一定要去影院多多支持！</p><p>虽然长大了，但还是那群被选召的孩子啊！</p><p><img src="https://i.loli.net/2020/10/31/LGVBxJi3PK2k4m1.png" alt="image-20201031181706617"></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>TextCNN</title>
      <link href="2020/10/15/TextCNN/"/>
      <url>2020/10/15/TextCNN/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>CNN的全称是Convolutional Neural Networks，中文名称是卷积神经网络。通过卷积的方式对于图像文字进行识别。</p><p>本文主要介绍Text CNN的原理和代码实现（超级详解）！</p><h1 id="Text-CNN的原理"><a href="#Text-CNN的原理" class="headerlink" title="Text CNN的原理"></a>Text CNN的原理</h1><h2 id="矩阵是怎样炼成的"><a href="#矩阵是怎样炼成的" class="headerlink" title="矩阵是怎样炼成的"></a>矩阵是怎样炼成的</h2><p>对于图片进行CNN的图像识别，首先是将图片进行矩阵化，像素化，图像本质上是一个矩阵。如果是黑白图像，则本质上就是一个矩阵，矩阵上的每一个元素为从0到255间取值。</p><p>将图片还原成一个矩阵之后则可以通过CNN的卷积池化等基本操作来对于图片进行分类识别了。</p><p>对于Text CNN也是一样的。我们尝试着让训练后的模型能够识别出这段文字的类别。比如，将新闻分为政治新闻，财经新闻，体育新闻，娱乐新闻等等。类比于图片，我们需要为一句话构建一个像图片一样的矩阵。但是怎么做到呢？<strong>词向量</strong>！</p><h2 id="词向量的爱恨情仇"><a href="#词向量的爱恨情仇" class="headerlink" title="词向量的爱恨情仇"></a>词向量的爱恨情仇</h2><p>所以何谓词向量？首先由于这个算法是歪果仁发明的，而英语采用的是一个词一个词进行分类的，那么对于中国人来说，我们的语句构成的基本单位是字，所以之后虽然说的是词向量，我们其实是按照词进行的标识。（当然，你也可以进行词分类，Github上有成型的进行分词的代码模型，叫做<a href="https://github.com/fxsjy/jieba">结巴分词</a>，结巴分词可以将一句话以词当作基本单位进行构建词向量，理论上讲语句分类效果应该更好，但我没有实践过，纯属推测。）</p><p>当你构建词向量之后，在脑海中想象一下，将一句话竖过来，那么我们可以构建一个矩阵，矩阵每行对应着一个词向量。当我们构建好这个矩阵之后，即可进行神经卷积了！</p><p>真不知道是哪个大佬真聪明智慧，想到的这种方法，解决了多少莘莘学子（误。</p><h2 id="卷积神经网络的构建"><a href="#卷积神经网络的构建" class="headerlink" title="卷积神经网络的构建"></a>卷积神经网络的构建</h2><p>卷积神经网络，顾名思义，最重要的就是卷积，何为卷积？个人理解：卷，就是内卷（误。</p><p>卷积，主要是积，我们设置一个三维的卷积核（也有描述为一个卷积核为一个矩阵，共有多少个卷积核，即通道数，channel数）。让一个卷积核和构建的句子的矩阵相乘，从左往右，从上往下，能够构建出一个新的小矩阵。</p><p>池化，池化为缩小矩阵，通过例如取最大值的方式进行。</p><h1 id="Text-CNN的实现"><a href="#Text-CNN的实现" class="headerlink" title="Text CNN的实现"></a>Text CNN的实现</h1><h2 id="源代码拷贝"><a href="#源代码拷贝" class="headerlink" title="源代码拷贝"></a>源代码拷贝</h2><p>代码来自于：<a href="https://github.com/649453932/Chinese-Text-Classification-Pytorch">Chinese-Text-Classification-Pytorch</a></p><p>代码通过pytorch进行神经卷积的构建</p><p>我没有怎么改动代码，主要是把结构捋顺，然后注释写的比较详细，很适合像我一样的菜鸡阅读，用jupyter notebook呈现出来。</p><p>这是我加了不少注释的版本：<a href="https://github.com/sjspingfandeshijie/NLP/blob/main/Chinese-Text-Classification-Pytorch/textCNN.ipynb">jupyter notebook</a></p><h2 id="代码详解"><a href="#代码详解" class="headerlink" title="代码详解"></a>代码详解</h2><h3 id="训练集，测试集简介"><a href="#训练集，测试集简介" class="headerlink" title="训练集，测试集简介"></a>训练集，测试集简介</h3><p>采用的测试集和训练集都是采用的清华大学的研究的数据集<a href="http://thuctc.thunlp.org/">THUCNews</a>，主要将新闻标题进行分类。分为10类。</p><p>规格如下：</p><p>​    中华女子学院：本科层次仅1专业招男生    3<br>​    两天价网站背后重重迷雾：做个网站究竟要多少钱    4<br>​    东5环海棠公社230-290平2居准现房98折优惠    1<br>​    卡佩罗：告诉你德国脚生猛的原因 不希望英德战踢点球    7<br>​    82岁老太为学生做饭扫地44年获授港大荣誉院士    5<br>​    记者回访地震中可乐男孩：将受邀赴美国参观    5<br>​    传郭晶晶欲落户香港战伦敦奥运 装修别墅当婚房    1<br>​    《赤壁OL》攻城战诸侯战硝烟又起    8<br>​    “手机钱包”亮相科博会    4</p><p>分类标准如下：</p><p>​    finance<br>​    realty<br>​    stocks<br>​    education<br>​    science<br>​    society<br>​    politics<br>​    sports<br>​    game<br>​    entertainment</p><h3 id="构建词汇和数据集"><a href="#构建词汇和数据集" class="headerlink" title="构建词汇和数据集"></a>构建词汇和数据集</h3><p>词汇的构建，首先需要保证训练的数据集中每个字要保证它的独特性，即我们需要给每个字进行一个独一无二的编号。采用的是UTF-8编码方式（这个编码比较全面，有些奇怪符号也能识别记录下来，这是我从另一门款爷的数据仓库中了解到的小知识，什么？你不知道款爷？<a href="https://sse.tongji.edu.cn/Data/View/3142">点击了解款爷</a>）。</p><p>其中第五行的for循环，主要是将数据集中的内容导入，清洗，包括去掉空格，去掉tab，然后进行字的筛选录入（如果已经存在就不导入）。这样，词汇的词典就构建完成了！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_vocab</span>(<span class="params">file_path, tokenizer, max_size, min_freq</span>):</span></span><br><span class="line">    <span class="comment">#词汇字典</span></span><br><span class="line">    vocab_dic = &#123;&#125;</span><br><span class="line">    <span class="keyword">with</span> open(file_path, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;UTF-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> tqdm(f):</span><br><span class="line">            lin = line.strip()</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> lin:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            content = lin.split(<span class="string">&#x27;\t&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">            <span class="keyword">for</span> word <span class="keyword">in</span> tokenizer(content):</span><br><span class="line">                vocab_dic[word] = vocab_dic.get(word, <span class="number">0</span>) + <span class="number">1</span></span><br><span class="line">        vocab_list = sorted([_ <span class="keyword">for</span> _ <span class="keyword">in</span> vocab_dic.items() <span class="keyword">if</span> _[<span class="number">1</span>] &gt;= min_freq], key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)[:max_size]</span><br><span class="line">        vocab_dic = &#123;word_count[<span class="number">0</span>]: idx <span class="keyword">for</span> idx, word_count <span class="keyword">in</span> enumerate(vocab_list)&#125;</span><br><span class="line">        vocab_dic.update(&#123;UNK: len(vocab_dic), PAD: len(vocab_dic) + <span class="number">1</span>&#125;)</span><br><span class="line">    <span class="keyword">return</span> vocab_dic</span><br></pre></td></tr></table></figure><p>数据集的构建，则将词语进行更多的操作。比如将每个字进行了出现频率上的统计。</p><p>同时构建三个数据集，分别是训练集，验证集，测试集。训练集用于训练模型，验证集用于每个epoch的验证，测试集用于最后测试效果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_dataset</span>(<span class="params">config, ues_word</span>):</span></span><br><span class="line">    <span class="keyword">if</span> ues_word:</span><br><span class="line">        tokenizer = <span class="keyword">lambda</span> x: x.split(<span class="string">&#x27; &#x27;</span>)  <span class="comment"># 以空格隔开，word-level</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        tokenizer = <span class="keyword">lambda</span> x: [y <span class="keyword">for</span> y <span class="keyword">in</span> x]  <span class="comment"># char-level</span></span><br><span class="line">    <span class="comment"># 判断并扩充词字典</span></span><br><span class="line">    <span class="keyword">if</span> os.path.exists(config.vocab_path):</span><br><span class="line">        vocab = pkl.load(open(config.vocab_path, <span class="string">&#x27;rb&#x27;</span>))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        vocab = build_vocab(config.train_path, tokenizer=tokenizer, max_size=MAX_VOCAB_SIZE, min_freq=<span class="number">1</span>)</span><br><span class="line">        pkl.dump(vocab, open(config.vocab_path, <span class="string">&#x27;wb&#x27;</span>))</span><br><span class="line">    <span class="comment"># 多少个字词</span></span><br><span class="line">    print(<span class="string">f&quot;Vocab size: <span class="subst">&#123;len(vocab)&#125;</span>&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 加载数据集</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load_dataset</span>(<span class="params">path, pad_size=<span class="number">32</span></span>):</span></span><br><span class="line">        contents = []</span><br><span class="line">        <span class="keyword">with</span> open(path, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;UTF-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> tqdm(f):     <span class="comment"># tqdm进度条</span></span><br><span class="line">                lin = line.strip()   <span class="comment"># strip()去掉了首尾的空格</span></span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> lin:          <span class="comment">#判断不是空的</span></span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                content, label = lin.split(<span class="string">&#x27;\t&#x27;</span>)    <span class="comment"># content去掉tab</span></span><br><span class="line">                words_line = []</span><br><span class="line">                token = tokenizer(content)          <span class="comment"># tokenizer为分词器，token变成不同数字统计类似，模式示例&#123;&#x27;下&#x27;: 1, &#x27;北京&#x27;: 1, &#x27;今天&#x27;: 2, &#x27;雨&#x27;: 1, &#x27;了&#x27;: 2, &#x27;我&#x27;: 1, &#x27;加班&#x27;: 1&#125;</span></span><br><span class="line">                seq_len = len(token)                <span class="comment"># token的长度，多少个词</span></span><br><span class="line">                <span class="comment"># 之前没有的就扩充</span></span><br><span class="line">                <span class="keyword">if</span> pad_size:</span><br><span class="line">                    <span class="keyword">if</span> len(token) &lt; pad_size:</span><br><span class="line">                        token.extend([PAD] * (pad_size - len(token)))</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        token = token[:pad_size]</span><br><span class="line">                        seq_len = pad_size</span><br><span class="line">                <span class="comment"># word to id</span></span><br><span class="line">                <span class="keyword">for</span> word <span class="keyword">in</span> token:</span><br><span class="line">                    words_line.append(vocab.get(word, vocab.get(UNK)))</span><br><span class="line">                contents.append((words_line, int(label), seq_len))</span><br><span class="line">        <span class="keyword">return</span> contents  <span class="comment"># [([...], 0), ([...], 1), ...]</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 三种数据集的加载</span></span><br><span class="line">    train = load_dataset(config.train_path, config.pad_size) <span class="comment"># 训练集集</span></span><br><span class="line">    dev = load_dataset(config.dev_path, config.pad_size) <span class="comment"># 验证集</span></span><br><span class="line">    test = load_dataset(config.test_path, config.pad_size) <span class="comment"># 测试集</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#返回的是词典，三种数据集</span></span><br><span class="line">    <span class="keyword">return</span> vocab, train, dev, test</span><br></pre></td></tr></table></figure><h3 id="构建词向量"><a href="#构建词向量" class="headerlink" title="构建词向量"></a>构建词向量</h3><p>词向量通过Embedding函数进行，如前文所述，完成词向量的构建。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        super(Model, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> config.embedding_pretrained <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.embedding = nn.Embedding.from_pretrained(config.embedding_pretrained, freeze=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.embedding = nn.Embedding(config.n_vocab, config.embed, padding_idx=config.n_vocab - <span class="number">1</span>)<span class="comment">#Embedding函数（词的数量，</span></span><br><span class="line">        self.convs = nn.ModuleList(</span><br><span class="line">            [nn.Conv2d(<span class="number">1</span>, config.num_filters, (k, config.embed)) <span class="keyword">for</span> k <span class="keyword">in</span> config.filter_sizes])</span><br><span class="line">        self.dropout = nn.Dropout(config.dropout)</span><br><span class="line">        self.fc = nn.Linear(config.num_filters * len(config.filter_sizes), config.num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 卷积和池化</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">conv_and_pool</span>(<span class="params">self, x, conv</span>):</span></span><br><span class="line">        x = F.relu(conv(x)).squeeze(<span class="number">3</span>) <span class="comment"># 卷积</span></span><br><span class="line">        x = F.max_pool1d(x, x.size(<span class="number">2</span>)).squeeze(<span class="number">2</span>) <span class="comment"># 池化</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment"># （嵌入层），数量，维度</span></span><br><span class="line">        out = self.embedding(x[<span class="number">0</span>])</span><br><span class="line">        out = out.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        out = torch.cat([self.conv_and_pool(out, conv) <span class="keyword">for</span> conv <span class="keyword">in</span> self.convs], <span class="number">1</span>)</span><br><span class="line">        out = self.dropout(out) <span class="comment"># 随机失活</span></span><br><span class="line">        out = self.fc(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure><p>然后就是CNN的固定操作了，卷积，池化，吧啦吧啦。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 卷积和池化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_and_pool</span>(<span class="params">self, x, conv</span>):</span></span><br><span class="line">    x = F.relu(conv(x)).squeeze(<span class="number">3</span>) <span class="comment"># 卷积</span></span><br><span class="line">    x = F.max_pool1d(x, x.size(<span class="number">2</span>)).squeeze(<span class="number">2</span>) <span class="comment"># 池化</span></span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="2020/10/02/hello-world/"/>
      <url>2020/10/02/hello-world/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>Hello world!</p><p>This is my first blog.</p><p>And this is me:</p><p><img src="https://i.loli.net/2020/10/16/qd9Fxea4t1IjHUW.jpg" alt="sjs"></p><p>Hope you enjoy your time in my blog.</p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
